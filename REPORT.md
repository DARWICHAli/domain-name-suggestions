# Rapport ‚Äì Fine-tuning LLM pour la g√©n√©ration de noms de domaine

## 1. Introduction
Ce projet a pour objectif de fine-tuner un mod√®le de langage afin de g√©n√©rer
des listes de **10 noms de domaine** valides, pertinents et s√ªrs √† partir d‚Äôune description d‚Äôentreprise.

L‚Äôapproche suivie repose sur :
- la pr√©paration d‚Äôun dataset synth√©tique,
- l‚Äôutilisation de LoRA/QLoRA pour un fine-tuning efficace,
- l‚Äô√©valuation syst√©matique via un holdout set et des edge cases.

---

## 2. Pipeline global
Le pipeline de traitement est r√©sum√© ci-dessous :

[Synth√®se donn√©es] 
    ‚Üí [Split train/valid/holdout] 
    ‚Üí [Fine-tuning (QLoRA)] 
    ‚Üí [√âvaluation holdout + edge cases] 
    ‚Üí [Comparaison des exp√©riences]



---

## 3. Exp√©riences men√©es

```text
### Exp√©rience 1 ‚Äì Baseline
- **Mod√®le** : Llama-3.2-8B-Instruct (QLoRA)
- **Dataset** : train.jsonl
- **Config** : LoRA (r=16, Œ±=32), 1 epoch, batch_size=2
- **But** : √©tablir un point de r√©f√©rence.

### Exp√©rience 2 ‚Äì Dataset enrichi + QLoRA
- **Mod√®le** : Llama-3.2-3B-Instruct
- **Dataset** : train_v1.jsonl (int√©grant 20% d‚Äôedge cases)
- **Config** : LoRA (r=16, Œ±=32), 4 epochs, batch_size=4
- **But** : am√©liorer robustesse et s√©curit√©.

### Exp√©rience 3A ‚Äì Hyperparam√®tres (epochs ‚Üì, lr ‚Üì)
- **Mod√®le** : Llama-3.2-3B-Instruct
- **Dataset** : train_v1.jsonl
- **Config** : LoRA (r=16, Œ±=32), 2 epochs, lr=1e-5, batch_size=6
- **But** : tester la g√©n√©ralisation avec moins d‚Äôepochs.

### Exp√©rience 3B ‚Äì LoRA plus expressif
- **Mod√®le** : Llama-3.2-3B-Instruct
- **Dataset** : train_v1.jsonl
- **Config** : LoRA (r=32, Œ±=64), 2 epochs, lr=2e-5, batch_size=4
- **But** : accro√Ætre la capacit√© d‚Äôadaptation.

### Exp√©rience 4 ‚Äì Mod√®le plus grand + plus d‚Äôepochs
- **Mod√®le** : Llama-3.1-8B-Instruct
- **Dataset** : train_v1.jsonl
- **Config** : LoRA (r=16, Œ±=32), 10 epochs, batch_size=1
- **But** : Augmenter le nombre de params, et l'epochs.

```
---

## 4. R√©sultats comparatifs

| Exp√©rience | Mod√®le                   | Dataset        | Epochs | LoRA (r, Œ±)  | Loss|mean_token_accuracy| Score Holdout | Score Edge | Safety Pass % | 
|------------|-------------------------------|-----------------|-------------|-----------------|----|-------------------|---------------|------------|---------------|
| Exp 1  | Llama-3.2-8B-Instruct (QLoRA) | train.jsonl    | 1  | (16,32) | 0.1901 | 0.9693 | 3.803 | 3.889 | 70% | 
| Exp 2  | Llama-3.2-3B-Instruct (QLoRA) | train_v1.jsonl | 4  | (16,32) | 0.2172 | 0.9675 | 3.792 | N/A | 50% |
| Exp 3A | Llama-3.2-3B-Instruct (QLoRA) | train_v1.jsonl | 2  | (16,32) | 0.2172 | 0.9589 | 3.775 | 3.841 | 80% |
| Exp 3B | Llama-3.2-3B-Instruct (QLoRA) | train_v1.jsonl | 2  | (32,64) | 0.2537 | 0.9663 | 3.839 | 3.9 | 80% |
| Exp 4  | Llama-3.1-8B-Instruct (QLoRA) |¬†train_v1.jsonl |¬†10 |¬†(16,32) | 0.1227 | 0.9814 | 3.873 | 3.989 | 80% |

---

## 5. Analyse

- **Exp1 (baseline)** : fournit un premier mod√®le fonctionnel, mais avec un **taux de s√©curit√© faible (70%)** et des scores modestes.  
- **Exp2 (dataset enrichi, 4 epochs)** : l√©g√®re baisse du taux de s√©curit√© (50%), probablement li√©e √† un **sur-apprentissage** ; le dataset enrichi n‚Äôa pas apport√© l‚Äôam√©lioration attendue avec ces hyperparam√®tres.  
- **Exp3A (2 epochs, lr plus faible)** : meilleure g√©n√©ralisation, **taux de s√©curit√© √† 80%** avec un compromis int√©ressant entre qualit√© et robustesse.  
- **Exp3B (LoRA plus large, r=32, Œ±=64)** : am√©lioration nette des scores sur **holdout et edge cases**, tout en maintenant la s√©curit√© √† 80%. Cela montre que l‚Äôaugmentation de la capacit√© des adaptateurs aide le mod√®le √† mieux apprendre sans d√©grader la stabilit√©.  
- **Exp4 (8B, 10 epochs)** : meilleurs r√©sultats globaux sur toutes les m√©triques, mais au prix d‚Äôun **co√ªt m√©moire √©lev√©** et d‚Äôun **risque d‚Äôoverfit**. Ce setup est moins pratique pour une utilisation courante.  


---

## 6. Conclusion
En conclusion :  
- Le **dataset enrichi seul (Exp2)** n‚Äôa pas suffi : il doit √™tre combin√© √† un choix judicieux d‚Äôhyperparam√®tres.  
- Les **configurations LoRA plus larges (Exp3B)** apportent un gain r√©el en qualit√© et s√©curit√© tout en restant abordables en ressources.  
- L‚Äôentra√Ænement long sur un mod√®le plus grand (**Exp4**) donne les meilleurs r√©sultats absolus, mais il est co√ªteux et peu scalable.  

üëâ Ainsi, **le meilleur compromis est Exp3B**, qui combine de bonnes performances, une s√©curit√© √©lev√©e (80%), et un co√ªt d‚Äôentra√Ænement raisonnable.  
**Exp4** reste la meilleure configuration en absolu, mais ne serait recommand√©e que si les ressources (GPU/temps) ne sont pas une contrainte.  

---

## 7. Limites et perspectives

### Limites
- **Dataset** : les donn√©es utilis√©es sont en grande partie synth√©tiques, ce qui peut limiter la diversit√© et la repr√©sentativit√© des cas r√©els.  
- **√âvaluation** : les m√©triques reposent principalement sur des heuristiques et un LLM-judge, sans v√©ritable √©valuation humaine de la pertinence ou de la cr√©ativit√© des noms g√©n√©r√©s.  
- **Overfitting** : certains r√©glages (Exp2 et Exp4) montrent une tendance √† l‚Äôoverfit, ce qui r√©duit la capacit√© du mod√®le √† g√©n√©raliser.  

### Perspectives
- **Donn√©es r√©elles** : collecter un corpus plus large et plus vari√© de descriptions d‚Äôentreprises r√©elles afin d‚Äôam√©liorer la robustesse.  
- **√âvaluation avanc√©e** : mettre en place une √©valuation humaine et/ou bas√©e sur des crit√®res business (ex. pertinence marketing, disponibilit√© r√©elle des noms de domaine).  
- **Optimisation m√©moire** : explorer d‚Äôautres techniques de fine-tuning efficaces en ressources (ex. LoRA + quantization mixte, adapters sp√©cifiques).  
- **RLHF / RLAIF** : int√©grer une √©tape de Reinforcement Learning from Human/AI Feedback pour aligner le mod√®le sur des pr√©f√©rences de qualit√© et de s√©curit√© plus fines.  


En d√©finitive, ce projet a montr√© qu‚Äôun fine-tuning l√©ger (QLoRA + LoRA) permet d‚Äôadapter efficacement des mod√®les de grande taille √† une t√¢che sp√©cifique, tout en √©quilibrant performance, s√©curit√© et contraintes de ressources. Les perspectives ouvertes laissent entrevoir des applications concr√®tes et extensibles dans des contextes r√©els.
