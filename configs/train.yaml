model_name: meta-llama/Llama-3.1-8B-Instruct 
output_dir: artifacts/model_v4
logging_dir: artifacts/logs
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: [q_proj, v_proj]

training:
  learning_rate: 2e-5
  batch_size: 1
  gradient_accumulation_steps: 8
  num_train_epochs: 10
  seed: 42
  bf16: true
  gradient_checkpointing: true
  qlora: true

data:
  #train_file: data/processed/train.jsonl
  train_file: data/processed/train_v1.jsonl
  val_file: data/processed/valid.jsonl
 